project one：骑手运营业务鸿蒙（HarmonyOS NEXT）全栈适配专项
骑手运营业务鸿蒙（HarmonyOS NEXT）全栈适配专项-阶段一：MRN 业务标准化迁移（底层治理）（2023-2024）
业务背景： 随着美团骑手端向鸿蒙系统演进，原有业务存在大量非标的 KNB 基础桥、自定义CSI业务桥及容器。这些历史债务导致鸿蒙适配成本极高，且各业务方受常规需求挤压，迁移人力严重不足。
核心任务： 主导106个业务Bundle的标准化治理，将非标协议、组件、容器向标准化MSI桥、标准MRN容器、组件收窄，在低成本适配的同时保障百万级骑手在线业务的稳定性。
关键行动：
锁范围与建基建： 盘点全业务非标依赖，设计并封装MSI标准化中间层，通过接口出入参一致化处理，磨平新老桥协议差异，降低业务改动成本。
提效工具研发： 引入自动化扫描脚本，实现CSI/KNB桥调用点的秒级识别与输出配套的迁移方案，目前已应用至所有业务迁移中。
流程管控与双分支： 制定双分支并行发布规范，确保标准化分支与Master物理隔离，通过CI/CD校验版本依赖，保障迭代过程中风险不交叉。
稳定性兜底： 建立交互成功率、页面加载率、MSI桥成功率等技术指标大盘，并配合“小黄条”及容器维度开关（Diva）实现降级回滚能力。
项目结果： 累计完成 100% (106/106) 的标准化迁移，实现业务自定义容器/组件 100% 收窄，自定义桥接口减少 35%。在保障迁移进程 0 事故 的前提下，为阶段二的鸿蒙原生适配扫清了架构障碍。

骑手运营业务鸿蒙（HarmonyOS NEXT）全栈适配专项——阶段二：双技术栈适配与全链路稳态建设 (2025)
背景： 在完成标准化治理后，业务进入鸿蒙原生适配的关键期。由于鸿蒙系统单框架（NEXT）底层采用高版本 ReactNative (RN 0.72)，与 iOS/Android 现有的低版本 RN 存在严重的同名依赖冲突。若采用手动切换配置及重装依赖的传统方式，开发体验极差且易导致路径解析风险。同时，H5 业务原有的“皮卡丘”系统存在发布风险与监控盲区，无法满足三端隔离迭代的需求。
核心任务： 主导MRN与H5双技术栈在鸿蒙端的深度适配。核心目标包括：在同一套业务工程下实现跨平台依赖共存；实现 H5 业务与双端发布的物理隔离；构建覆盖全业务的核心流程监控体系，确保上线过程0级故障。
关键行动：
双平台构建环境搭建（双 package.json 隔离方案）：针对同名依赖冲突，深度应用平台研发的 @mrn/oh-cli (mrn-oh) 工具。通过在工程目录下定义 package.json（双端低版本）与 oh-package.json（鸿蒙高版本）两套配置，利用该 CLI 实现一键切换依赖并自动恢复环境，解决了双平台构建时的开发体验痛点，且无需修改 node_modules 结构，完美兼容原有 Metro 等工具链。
H5发布架构重构（解耦发布）：将 H5 工程从皮卡丘系统迁移至 Talos 专属流水线。通过独立构建与灰度路径，彻底解决发布耦合风险，确保鸿蒙端具备独立回滚能力。
底层桥接能力对标：主导注册 url_set_id 以标识容器接入；升级中间适配层，在鸿蒙端利用 MSI 桥 兼容传统的 KNB 调用，有效降低了业务方的适配改造工作量。
稳定性防御体系：落实**“七要”质量准则**，圈定 30 个核心 H5 页面及 57 个核心 MRN Bundle 进行深度 Review。建立基于 Logan+Raptor 的鸿蒙维度监控大盘，覆盖容器加载成功率、业务桥调用成功率及 JS Error 等关键指标。
运营闭环与反馈：搭建日常问题反馈机制，并建立一线骑手反馈渠道，由业务负责人主动 Check 功能完备性，快速推进全量适配目标。
结果： 累计交付 112 个 MRN Bundle 与 39 个核心 H5 页面 的鸿蒙化适配。通过Talos流水线实现了H5鸿蒙端的独立发布与风险隔离。项目整体在保障骑手端首发的同时，构建了从环境管理、工程发布到线上观测的全链路治理闭环，确保了鸿蒙演进过程中的业务确定性。

project three：京东 PLUS 会员权益业务架构重构与配置化升级
背景： 原PLUS会员权益业务采用“一权益一页面”的硬编码模式，架构极其僵化。每新增一项会员权益均需重复开发，且文案及数据的微调依赖频繁的发版上线，严重制约了业务的迭代效率与响应速度。
任务： 打破原有架构瓶颈，通过技术手段实现功能的跨页面复用，降低研发成本与上线频次，建立一套可支撑高频业务变动的配置化架构方案。
行动：
架构重构设计： 确立“前端组件化 + 数据接口配置化”的演进路线。将权益页面深度拆解，并与产品达成协议，将所有易变动数据收拢至接口下发。
组件体系建设： 搭建业务组件库，划分为基础UI组件与垂直业务组件，支持 Tree-shaking (按需加载) 以优化性能；同步封装公共工具函数库，通过 NPM 私有包 进行版本化维护。
交付模式变革： 抽象出标准化的权益模版，新权益接入仅需通过后台配置参数即可完成页面组装，极大降低了逻辑耦合。
结果： 新权益业务的开发交付效率提升近2倍。通过配置化手段显著降低了版本上线频次，实现了业务的快速响应，该架构模式后续被推广至团队内多个同类业务场景。

project four：全自动Web性能监测与质量控制平台
背景： 团队内部缺乏对线上页面用户体验的持续跟踪手段，性能优化往往停留在上线前的“三板斧”（资源优化、渲染优化、网络优化等），上线后的真实性能表现处于“监控盲区”，难以衡量长期的用户体验波动。
任务： 从零建设一套自动化的性能监测系统，实现对业务页面性能指标的持续采集、分析与预警，通过数据驱动前端质量优化。
行动：
技术栈决策： 调研业界方案后，选定 Puppeteer + Lighthouse 作为底层核心（利用 Puppeteer 强大的社区生态与认证模拟能力）。后端基于 Node.js (Express) + TypeORM + MySQL 搭建，前端采用 React + TS 构建可视化大盘。
核心功能研发： 实现了携带认证 Token 的自动化爬取任务，支持定时采集 FCP、LCP、CLS、TTI 等 Core Web Vitals 核心指标，并进行多维度均值分析。
质量红线机制： 引入 Performance Budget (性能预算) 机制，支持针对不同业务线设置预算阈值。系统自动过滤并标记超标页面，直接对标 KPI 进行预警。
结果： 该工具成功覆盖了 PLUS 会员核心业务页面，产出了数十份性能分析报告。通过性能预算机制强制推行质量红线，显著提升了团队的性能意识，带动业务页面平均 LCP 提升显著，实现了前端质量的闭环管理。

